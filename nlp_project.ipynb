{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* the pdf with your answers\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Python 3.6 or above is required\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import difflib\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path('C:/Users/Armand/Desktop/3A/Deep Learning/nlp_project/nlp_project/')\n",
    "# Download word vectors, might take a few minutes and about ~3GB of storage space\n",
    "en_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\n",
    "if not en_embeddings_path.exists():\n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\n",
    "fr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\n",
    "if not fr_embeddings_path.exists():\n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "\n",
    "    def __init__(self, filepath, vocab_size=50000):\n",
    "        self.words, self.embeddings = self.load_wordvec(filepath, vocab_size)\n",
    "        # Mappings for O(1) retrieval:\n",
    "        self.word2id = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.id2word = {idx: word for idx, word in enumerate(self.words)}\n",
    "    \n",
    "    def load_wordvec(self, filepath, vocab_size):\n",
    "        assert str(filepath).endswith('.gz')\n",
    "        words = []\n",
    "        embeddings = []\n",
    "        with gzip.open(filepath, 'rt',encoding=\"utf8\") as f:  # Read compressed file directly\n",
    "            next(f)  # Skip header\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                words.append(word)\n",
    "                embeddings.append(np.fromstring(vec, sep=' '))\n",
    "                if i == (vocab_size - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(words)))\n",
    "        return words, np.vstack(embeddings)\n",
    "    \n",
    "    def encode(self, word):\n",
    "        # Returns the 1D embedding of a given word\n",
    "        #return self.embeddings[self.word2id[word]]\n",
    "        try:\n",
    "            i = self.word2id[word]\n",
    "            return self.embeddings[i]\n",
    "        except:\n",
    "            try:\n",
    "                word = difflib.get_close_matches(word, self.words)[0]\n",
    "                i = self.word2id[word]\n",
    "            except:\n",
    "                return 0\n",
    "        return self.embeddings[i]\n",
    "    \n",
    "    def score(self, word1, word2):\n",
    "        # Return the cosine similarity: use np.dot & np.linalg.norm\n",
    "        code1=self.encode(word1)\n",
    "        code2=self.encode(word2)\n",
    "        return np.dot(code1,code2)/(np.linalg.norm(code1)*np.linalg.norm(code2))\n",
    "    \n",
    "    def most_similar(self, word, k=5):\n",
    "        # Returns the k most similar words: self.score & np.argsort \n",
    "        \n",
    "        scores=[] #list of index and scores\n",
    "        for index,w2 in enumerate(self.words):\n",
    "            scores.append((self.score(word,w2),index))\n",
    "        sorted_scores=sorted(scores,reverse = True)\n",
    "        k_similar_index=[s[1] for s in sorted_scores[:k]]\n",
    "        k_similar_words=[self.words[i] for i in k_similar_index]\n",
    "        return k_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "cat tree 0.26449754661654756\n",
      "cat dog 0.7078641298542564\n",
      "cat pet 0.6753313359976382\n",
      "Paris France 0.6892958925806543\n",
      "Paris Germany 0.4051242286737549\n",
      "Paris baguette 0.29399958277802224\n",
      "Paris donut -0.006588507552348003\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'pup', 'canine']\n",
      "['dogs', 'dog', 'cats', 'puppies', 'Dogs']\n",
      "['Paris', 'France', 'Parisian', 'Marseille', 'Brussels']\n",
      "['Germany', 'Austria', 'Europe', 'Berlin', 'Hamburg']\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for word1, word2 in zip(('cat', 'cat', 'cat', 'Paris', 'Paris', 'Paris', 'Paris'), ('tree', 'dog', 'pet', 'France', 'Germany', 'baguette', 'donut')):\n",
    "    print(word1, word2, word2vec.score(word1, word2))\n",
    "for word in ['cat', 'dog', 'dogs', 'Paris', 'Germany']:\n",
    "    print(word2vec.most_similar(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BagOfWords():\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        # -> idf = {word: idf_value, ...}\n",
    "        idf={}\n",
    "        N=len(sentences)\n",
    "        \n",
    "        # get number of documents containing each word\n",
    "        for sentence in sentences:\n",
    "            wordsList=re.sub(\"[^\\w]\", \" \",sentence).split()\n",
    "            for word in set(wordsList):\n",
    "                idf[word]=idf.get(word, 0)+1\n",
    "                \n",
    "        #transform to get idf value of each word       \n",
    "        for word in idf:\n",
    "            idf[word]=np.log10(N/idf[word])\n",
    "        return idf\n",
    "        \n",
    "    \n",
    "    def encode(self, sentence, idf=None):\n",
    "        # Takes a sentence as input, returns the sentence embedding\n",
    "        wordsList=re.sub(\"[^\\w]\", \" \",sentence).split()\n",
    "        wordsVectors=[self.word2vec.encode(word) for word in wordsList]\n",
    "        if idf is None:\n",
    "            # mean of word vectors\n",
    "            return np.mean(wordsVectors,axis=0)\n",
    "        else:\n",
    "            # idf-weighted mean of word vectors\n",
    "            weightedMean=0\n",
    "            sumIdf=0\n",
    "            for i,word in enumerate(wordsList):\n",
    "                weightedMean+=idf.get(word,0)*wordsVectors[i]\n",
    "                sumIdf+=idf.get(word,0)\n",
    "            weightedMean=weightedMean/sumIdf\n",
    "            return weightedMean\n",
    "                                \n",
    "\n",
    "    def score(self, sentence1, sentence2, idf=None):\n",
    "        # cosine similarity: use np.dot & np.linalg.norm \n",
    "        code1=self.encode(sentence1,idf)\n",
    "        code2=self.encode(sentence2,idf)\n",
    "        return np.dot(code1,code2)/(np.linalg.norm(code1)*np.linalg.norm(code2))\n",
    "    \n",
    "    def most_similar(self, sentence, sentences, idf=None, k=5):\n",
    "        # Return most similar sentences\n",
    "        scores=[]\n",
    "        for sentence2 in sentences:\n",
    "            scores.append(self.score(sentence,sentence2,idf))\n",
    "            #see progression\n",
    "            if len(scores)%10000==0:\n",
    "                print(round(100*len(scores)/len(sentences)),\"% of scores computed\")\n",
    "        k_similar_index = np.argpartition(np.array(scores), -k)[-k:]\n",
    "        k_similar_index = k_similar_index[np.argsort([-scores[i] for i in k_similar_index])]\n",
    "        k_similar_sentences=[sentences[i] for i in k_similar_index]\n",
    "        return k_similar_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "\n",
      "\tAverage of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.6415914630303108\n",
      "7 % of scores computed\n",
      "13 % of scores computed\n",
      "20 % of scores computed\n",
      "27 % of scores computed\n",
      "33 % of scores computed\n",
      "40 % of scores computed\n",
      "46 % of scores computed\n",
      "53 % of scores computed\n",
      "60 % of scores computed\n",
      "66 % of scores computed\n",
      "73 % of scores computed\n",
      "80 % of scores computed\n",
      "86 % of scores computed\n",
      "93 % of scores computed\n",
      "100 % of scores computed\n",
      "1 smiling african american boy . \n",
      "1) 1 smiling african american boy . \n",
      "2) 2 woman dancing while pointing . \n",
      "3) 5 women and 1 man are smiling for the camera . \n",
      "4) 3 males and 1 woman enjoying a sporting event \n",
      "5) 2 chinese people wearing traditional clothes \n",
      "\n",
      "\tidf weighted average of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.6366631124436564\n",
      "7 % of scores computed\n",
      "13 % of scores computed\n",
      "20 % of scores computed\n",
      "27 % of scores computed\n",
      "33 % of scores computed\n",
      "40 % of scores computed\n",
      "46 % of scores computed\n",
      "53 % of scores computed\n",
      "60 % of scores computed\n",
      "66 % of scores computed\n",
      "73 % of scores computed\n",
      "80 % of scores computed\n",
      "86 % of scores computed\n",
      "93 % of scores computed\n",
      "100 % of scores computed\n",
      "2 chinese people wearing traditional clothes \n",
      "1) 2 chinese people wearing traditional clothes \n",
      "2) 5 children , 2 boys and 3 girls , share food while sitting outside of a building . \n",
      "3) 2 women in red outfits and 3 other people are standing at a street corner . \n",
      "4) 2 white ladies in casual clothing . \n",
      "5) 3 men in tuxedos and 2 women in dresses , sleeping on the grass . \n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "sentence2vec = BagOfWords(word2vec)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "filepath = PATH_TO_DATA / 'sentences.txt'\n",
    "with open(filepath, 'r') as f:\n",
    "    sentences = [line.strip('\\n') for line in f]\n",
    "\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('\\n\\tAverage of word embeddings')\n",
    "sentence1 = sentences[7]\n",
    "sentence2 = sentences[13]\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2))\n",
    "sentence = sentences[10]\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences)  # BagOfWords-mean\n",
    "print(sentence)\n",
    "for i, sentence in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = sentence2vec.build_idf(sentences)\n",
    "\n",
    "print('\\n\\tidf weighted average of word embeddings')\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2, idf))\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences, idf)  # BagOfWords-idf\n",
    "print(sentence)\n",
    "for i, sentence in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualWordAligner:\n",
    "    \n",
    "    def __init__(self, fr_word2vec, en_word2vec):\n",
    "        self.fr_word2vec = fr_word2vec\n",
    "        self.en_word2vec = en_word2vec\n",
    "        self.aligned_fr_embeddings = self.get_aligned_fr_embeddings()\n",
    "        \n",
    "    def get_aligned_fr_embeddings(self):\n",
    "        # 1 - Get words that appear in both vocabs (= identical character strings)\n",
    "        #     Use it to create the matrix X (emb_dim, vocab_size) and Y (emb_dim, vocab_size) (of embeddings for these words)\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        for word in self.fr_word2vec.words:\n",
    "            if word in self.en_word2vec.words:\n",
    "                if len(X)>0:\n",
    "                    X=np.hstack((X,np.array([self.fr_word2vec.encode(word)]).T))\n",
    "                    Y=np.hstack((Y,np.array([self.en_word2vec.encode(word)]).T))\n",
    "                else:\n",
    "                    X=np.array([self.fr_word2vec.encode(word)]).T\n",
    "                    Y=np.array([self.en_word2vec.encode(word)]).T\n",
    "        assert X.shape[0] == 300 and Y.shape[0] == 300\n",
    "        \n",
    "        # 2 - Solve the Procrustes using the numpy package and: np.linalg.svd() and get the optimal W\n",
    "        #     Now self.fr_word2vec.embeddings * W.transpose() is in the same space as en_word2vec.embeddings\n",
    "        U, s, Vt=np.linalg.svd(np.dot(Y,X.T))\n",
    "        W=np.dot(U,Vt)\n",
    "        assert W.shape == (300, 300)\n",
    "        return np.matmul(fr_word2vec.embeddings, W.transpose())\n",
    "        \n",
    "    def get_closest_english_words(self, fr_word, k=3):\n",
    "        # 3 - Return the top k English nearest neighbors to the input French word\n",
    "        #fr_word_emb = self.fr_word2vec.encode(fr_word)\n",
    "        #en_word_emb = np.dot(W,fr_word_emb.T)\n",
    "        en_word_emb=self.aligned_fr_embeddings[self.fr_word2vec.word2id[fr_word]]\n",
    "        \n",
    "        scores=[] #list of index and scores\n",
    "        for index,en_word2_emb in enumerate(self.en_word2vec.embeddings):\n",
    "            score=np.dot(en_word_emb,en_word2_emb)/(np.linalg.norm(en_word_emb)*np.linalg.norm(en_word2_emb))\n",
    "            scores.append((score,index))\n",
    "        sorted_scores=sorted(scores,reverse = True)\n",
    "        k_similar_index=[s[1] for s in sorted_scores[:k]]\n",
    "        k_similar_words=[self.en_word2vec.words[i] for i in k_similar_index]\n",
    "        return k_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n",
      "----------\n",
      "fr: \"chat\"\n",
      "en: \"cat\"\n",
      "en: \"kitten\"\n",
      "en: \"kitty\"\n",
      "----------\n",
      "fr: \"chien\"\n",
      "en: \"dog\"\n",
      "en: \"cat\"\n",
      "en: \"pet\"\n",
      "----------\n",
      "fr: \"voiture\"\n",
      "en: \"car\"\n",
      "en: \"vehicle\"\n",
      "en: \"automobile\"\n",
      "----------\n",
      "fr: \"zut\"\n",
      "en: \"oops\"\n",
      "en: \"Ah\"\n",
      "en: \"ah\"\n"
     ]
    }
   ],
   "source": [
    "fr_word2vec = Word2Vec(fr_embeddings_path, vocab_size=50000)\n",
    "en_word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "multilingual_word_aligner = MultilingualWordAligner(fr_word2vec, en_word2vec)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "fr_words = ['chat', 'chien', 'voiture', 'zut']\n",
    "k = 3\n",
    "for fr_word in fr_words:\n",
    "    print('-' * 10)\n",
    "    print(f'fr: \"{fr_word}\"')\n",
    "    en_words = multilingual_word_aligner.get_closest_english_words(fr_word, k=3)\n",
    "    for en_word in en_words:\n",
    "        print(f'en: \"{en_word}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "train_filepath ='SST/stsa.fine.train'\n",
    "dev_filepath ='SST/stsa.fine.dev'\n",
    "test_filepath ='SST/stsa.fine.test.X'\n",
    "\n",
    "def read_file(path,is_test=False):\n",
    "    with open(path, \"r\") as p:\n",
    "        if is_test==False:\n",
    "            lines = p.read().split(\"\\n\")[:-1]\n",
    "            labels = np.array([int(line[0]) for line in lines])\n",
    "            sentences = [line[2:] for line in lines]\n",
    "            return sentences, labels\n",
    "        else:\n",
    "            sentences = p.read().split(\"\\n\")[:-1]\n",
    "            return sentences\n",
    "\n",
    "train_sentences=read_file(train_filepath)[0]\n",
    "train_labels=read_file(train_filepath)[1]\n",
    "dev_sentences=read_file(dev_filepath)[0]\n",
    "dev_labels=read_file(dev_filepath)[1]\n",
    "test_sentences=read_file(test_filepath,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armand\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "sentence2vec = BagOfWords(word2vec)\n",
    "\n",
    "#idf weighted embeddings\n",
    "idf = sentence2vec.build_idf(train_sentences+dev_sentences)\n",
    "train_sentences_emb=[sentence2vec.encode(train_sentences[i],idf) for i in range(len(train_sentences))]\n",
    "dev_sentences_emb=[sentence2vec.encode(dev_sentences[i],idf) for i in range(len(dev_sentences))]\n",
    "test_sentences_emb=[sentence2vec.encode(test_sentences[i],idf) for i in range(len(test_sentences))]\n",
    "\n",
    "#mean of words embeddings\n",
    "train_sentences_mean_emb=[sentence2vec.encode(train_sentences[i]) for i in range(len(train_sentences))]\n",
    "dev_sentences_mean_emb=[sentence2vec.encode(dev_sentences[i]) for i in range(len(dev_sentences))]\n",
    "test_sentences_mean_emb=[sentence2vec.encode(test_sentences[i]) for i in range(len(test_sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for idf-weighted BoW: \n",
      " Best value for the penalty: 10.0 \n",
      " Dev accuracy: 0.3923705722070845 \n",
      " Train accuracy: 0.4648876404494382\n",
      "Results for mean BoW: \n",
      " Best value for the penalty: 10.0 \n",
      " Dev accuracy: 0.3941871026339691 \n",
      " Train accuracy: 0.4598548689138577\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "#     In the paper, the accuracy for average of word vectors is 32.7%\n",
    "#     (VecAvg, table 1, https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#idf weighted embeddings\n",
    "train_acc,dev_acc=[],[]\n",
    "pen_values = 10.0**(np.arange(-2,2,0.5))\n",
    "\n",
    "for pen in pen_values:\n",
    "    logReg = LogisticRegression(penalty=\"l2\",C = pen,multi_class='auto',solver='newton-cg')\n",
    "    logReg.fit(train_sentences_emb, train_labels)\n",
    "    train_acc.append(logReg.score(train_sentences_emb, train_labels))\n",
    "    dev_acc.append(logReg.score(dev_sentences_emb, dev_labels))\n",
    "\n",
    "best_pen=pen_values[np.argmax(dev_acc)]\n",
    "best_train_acc=train_acc[np.argmax(dev_acc)]\n",
    "best_dev_acc=max(dev_acc)\n",
    "\n",
    "print(\"Results for idf-weighted BoW: \\n\",\"Best value for the penalty:\",best_pen,'\\n Dev accuracy:',best_dev_acc,'\\n Train accuracy:',best_train_acc)\n",
    "\n",
    "#mean of words embeddings\n",
    "train_acc,dev_acc=[],[]\n",
    "\n",
    "for pen in pen_values:\n",
    "    logReg = LogisticRegression(penalty=\"l2\",C = pen, multi_class='auto',solver='newton-cg')\n",
    "    logReg.fit(train_sentences_mean_emb, train_labels)\n",
    "    train_acc.append(logReg.score(train_sentences_mean_emb, train_labels))\n",
    "    dev_acc.append(logReg.score(dev_sentences_mean_emb, dev_labels))\n",
    "\n",
    "best_pen=pen_values[np.argmax(dev_acc)]\n",
    "best_train_acc=train_acc[np.argmax(dev_acc)]\n",
    "best_dev_acc=max(dev_acc)\n",
    "\n",
    "print(\"Results for mean BoW: \\n\",\"Best value for the penalty:\",best_pen,'\\n Dev accuracy:',best_dev_acc,'\\n Train accuracy:',best_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "#train our best model on all the data\n",
    "logReg = LogisticRegression(penalty=\"l2\",C = best_pen, multi_class='auto',solver='newton-cg')\n",
    "logReg.fit(train_sentences_mean_emb+dev_sentences_mean_emb, list(train_labels)+list(dev_labels))\n",
    "\n",
    "#make predictions\n",
    "label_predictions=logReg.predict(test_sentences_mean_emb)\n",
    "np.savetxt('logreg_bov_y_test_sst.txt', label_predictions, fmt= '%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for idf-weighted BoW: \n",
      " Best value for nb of leaves: 25 \n",
      " Dev accuracy: 0.3905540417801998\n",
      "Results for idf-weighted BoW: \n",
      " Best value for nb of leaves: 35 \n",
      " Dev accuracy: 0.3678474114441417\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# I try LGBM classifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#idf weighted embeddings\n",
    "train_acc,dev_acc=[],[]\n",
    "num_leaves_values = np.arange(15,40,5)\n",
    "\n",
    "for leaves in num_leaves_values:\n",
    "    clf = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=100,num_leaves=leaves)\n",
    "    clf.fit(train_sentences_emb, train_labels)\n",
    "    train_acc.append(clf.score(train_sentences_emb, train_labels))\n",
    "    dev_acc.append(clf.score(dev_sentences_emb, dev_labels))\n",
    "\n",
    "best_nb_leaves=num_leaves_values[np.argmax(dev_acc)]\n",
    "best_train_acc=train_acc[np.argmax(dev_acc)]\n",
    "best_dev_acc=max(dev_acc)\n",
    "\n",
    "print(\"Results for idf-weighted BoW: \\n\",\"Best value for nb of leaves:\",best_nb_leaves,'\\n Dev accuracy:',best_dev_acc)\n",
    "\n",
    "#mean of words embeddings\n",
    "train_acc,dev_acc=[],[]\n",
    "\n",
    "for leaves in num_leaves_values:\n",
    "    clf = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=100,num_leaves=leaves)\n",
    "    clf.fit(train_sentences_mean_emb, train_labels)\n",
    "    train_acc.append(clf.score(train_sentences_mean_emb, train_labels))\n",
    "    dev_acc.append(clf.score(dev_sentences_mean_emb, dev_labels))\n",
    "\n",
    "best_nb_leaves=num_leaves_values[np.argmax(dev_acc)]\n",
    "best_train_acc=train_acc[np.argmax(dev_acc)]\n",
    "best_dev_acc=max(dev_acc)\n",
    "\n",
    "print(\"Results for idf-weighted BoW: \\n\",\"Best value for nb of leaves:\",best_nb_leaves,'\\n Dev accuracy:',best_dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train our best model on all the data\n",
    "clf = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=100,num_leaves=leaves)\n",
    "clf.fit(train_sentences_mean_emb+dev_sentences_mean_emb, list(train_labels)+list(dev_labels))\n",
    "\n",
    "#make predictions\n",
    "label_predictions=clf.predict(test_sentences_mean_emb)\n",
    "np.savetxt('lgbm_bov_y_test_sst.txt', label_predictions, fmt= '%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Using the same dataset, transform text to integers using tf.keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "nb_words = 10000\n",
    "train_onehot = [one_hot(sentence, nb_words) for sentence in train_sentences]\n",
    "dev_onehot = [one_hot(sentence, nb_words) for sentence in dev_sentences]\n",
    "test_onehot = [one_hot(sentence, nb_words) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Pad your sequences using tf.keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "maxseqlen = max(map(len, train_onehot + dev_onehot + test_onehot))\n",
    "x_train = pad_sequences(train_onehot, padding = 'post', truncating = 'post', maxlen = maxseqlen)\n",
    "x_dev = pad_sequences(dev_onehot, padding = 'post', truncating = 'post', maxlen = maxseqlen)\n",
    "x_test = pad_sequences(test_onehot, padding = 'post', truncating = 'post', maxlen = maxseqlen)\n",
    "\n",
    "y_train = to_categorical(train_labels)\n",
    "y_dev = to_categorical(dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Design your encoder + classifier using tensorflow.keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this container : the lookup-table, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "#     Note that the embedding layer is initialized randomly and does not take advantage of pre-trained word embeddings.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 100  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 10000  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,042,565\n",
      "Trainable params: 1,042,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 4 - Define your loss/optimizer/metrics\n",
    "from keras import optimizers\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "opt      =  'rmsprop' #'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=opt,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/10\n",
      "8544/8544 [==============================] - 31s 4ms/sample - loss: 1.4531 - accuracy: 0.3512 - val_loss: 1.4964 - val_accuracy: 0.3315\n",
      "Epoch 2/10\n",
      "8544/8544 [==============================] - 27s 3ms/sample - loss: 1.4338 - accuracy: 0.3743 - val_loss: 1.4805 - val_accuracy: 0.3678\n",
      "Epoch 3/10\n",
      "8544/8544 [==============================] - 27s 3ms/sample - loss: 1.4241 - accuracy: 0.3944 - val_loss: 1.4799 - val_accuracy: 0.3869\n",
      "Epoch 4/10\n",
      "8544/8544 [==============================] - 30s 3ms/sample - loss: 1.4159 - accuracy: 0.3929 - val_loss: 1.4665 - val_accuracy: 0.3833\n",
      "Epoch 5/10\n",
      "8544/8544 [==============================] - 31s 4ms/sample - loss: 1.3883 - accuracy: 0.4086 - val_loss: 1.4340 - val_accuracy: 0.3315\n",
      "Epoch 6/10\n",
      "8544/8544 [==============================] - 25s 3ms/sample - loss: 1.3558 - accuracy: 0.4263 - val_loss: 1.4417 - val_accuracy: 0.3915\n",
      "Epoch 7/10\n",
      "8544/8544 [==============================] - 24s 3ms/sample - loss: 1.3304 - accuracy: 0.4375 - val_loss: 1.4476 - val_accuracy: 0.3769\n",
      "Epoch 8/10\n",
      "8544/8544 [==============================] - 25s 3ms/sample - loss: 1.3022 - accuracy: 0.4464 - val_loss: 1.4703 - val_accuracy: 0.3615\n",
      "Epoch 9/10\n",
      "8544/8544 [==============================] - 24s 3ms/sample - loss: 1.2652 - accuracy: 0.4581 - val_loss: 1.5111 - val_accuracy: 0.3815\n",
      "Epoch 10/10\n",
      "8544/8544 [==============================] - 24s 3ms/sample - loss: 1.2377 - accuracy: 0.4659 - val_loss: 1.4130 - val_accuracy: 0.3887\n"
     ]
    }
   ],
   "source": [
    "# 5 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "#     Keras expects y_train and y_dev to be one-hot encodings of the labels, i.e. with shape=(n_samples, 5)\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 16\n",
    "n_epochs = 10\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "#we use the last model because it was the one with the lowest validation loss\n",
    "y_test = model.predict(x_test)\n",
    "text = \"\\n\".join(y_test.max(axis=1).astype(str))\n",
    "file = open(\"logreg_lstm_y_test_sst.txt\", \"w\")\n",
    "file.write(text)\n",
    "file.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Dev accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 pretrained word vectors\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 3,093,765\n",
      "Trainable params: 93,765\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/5\n",
      "8544/8544 [==============================] - 38s 4ms/sample - loss: 1.5752 - accuracy: 0.2706 - val_loss: 1.5722 - val_accuracy: 0.2625\n",
      "Epoch 2/5\n",
      "8544/8544 [==============================] - 31s 4ms/sample - loss: 1.5700 - accuracy: 0.2622 - val_loss: 1.5727 - val_accuracy: 0.2534\n",
      "Epoch 3/5\n",
      "8544/8544 [==============================] - 32s 4ms/sample - loss: 1.5696 - accuracy: 0.2646 - val_loss: 1.5741 - val_accuracy: 0.2534\n",
      "Epoch 4/5\n",
      "8544/8544 [==============================] - 29s 3ms/sample - loss: 1.5690 - accuracy: 0.2722 - val_loss: 1.5786 - val_accuracy: 0.2534\n",
      "Epoch 5/5\n",
      "8544/8544 [==============================] - 29s 3ms/sample - loss: 1.5690 - accuracy: 0.2658 - val_loss: 1.5734 - val_accuracy: 0.2534\n"
     ]
    }
   ],
   "source": [
    "# 7 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# I tried to pretrain the lookup table, but the performance was bad\n",
    "\n",
    "#build embedding matrix with the embeddings from word2vec\n",
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=10000)\n",
    "\n",
    "embedding_matrix = np.zeros((10000, 300))\n",
    "for i, emb in enumerate(word2vec.embeddings):\n",
    "    embedding_matrix[i] = emb\n",
    "    \n",
    "#use the same model as before, with pretrained vectors\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 300  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 10000  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,embed_dim,weights=[embedding_matrix],trainable=False))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Defineloss/optimizer/metrics\n",
    "from keras import optimizers\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' \n",
    "opt      =  'adam'\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=opt,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())\n",
    "\n",
    "#train model\n",
    "bs = 16\n",
    "n_epochs = 5\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev, y_dev))\n",
    "\n",
    "#predictions\n",
    "y_test = model.predict(x_test)\n",
    "text = \"\\n\".join(y_test.max(axis=1).astype(str))\n",
    "file = open(\"pretrained_lstm_y_test_sst.txt\", \"w\")\n",
    "file.write(text)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
